{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1e147d9f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "import logging\n",
                "import csv\n",
                "import time\n",
                "import yaml\n",
                "import os\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "from src.tfl_client import *\n",
                "\n",
                "# Gets all lines and their valid routes for given modes, including the name and id of the originating and terminating stops for each route\n",
                "# GET https://api.tfl.gov.uk/Line/Mode/bus/Route\n",
                "\n",
                "# Get disruptions for all lines of the given modes.\n",
                "# GET https://api.tfl.gov.uk/Line/Mode/bus/Disruption\n",
                "\n",
                "# Get the list of arrival predictions for given line ids based at the given stop\n",
                "# GET https://api.tfl.gov.uk/Line/{lineId}/Arrivals\n",
                "\n",
                "def load_schema(path: str) -> dict:\n",
                "    with open(path, \"r\") as f:\n",
                "        return yaml.safe_load(f)\n",
                "\n",
                "def load_config(path: str) -> dict:\n",
                "    with open(path, \"r\") as f:\n",
                "        return json.load(f)\n",
                "\n",
                "def load_secrets():\n",
                "    app_id = os.getenv(\"TFL_APP_ID\")\n",
                "    app_key = os.getenv(\"TFL_APP_KEY\")\n",
                "\n",
                "    if not app_id or not app_key:\n",
                "        raise RuntimeError(\"Missing TfL credentials: set TFL_APP_ID and TFL_APP_KEY\")\n",
                "    return app_id, app_key\n",
                "\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "logger = logging.getLogger(__name__)\n",
                "app_id, app_key = load_secrets()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "86af32b7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-13 04:04:20,565 - INFO - Saved 1026 rows to data/reference/line_routes_snapshot/dt=2026-01-13/line_routes.csv\n"
                    ]
                }
            ],
            "source": [
                "# Preprocess the Line routes data\n",
                "snapshot_date = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
                "OUTPUT_DIR = Path(f\"data/reference/line_routes_snapshot/dt={snapshot_date}\")\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "OUTPUT_FILE = OUTPUT_DIR / \"line_routes.csv\"\n",
                "\n",
                "# Load config, schemas and params\n",
                "config = load_config(Path(\"config/routes.json\"))\n",
                "schema = load_schema(Path(\"schemas/reference/line_routes.yaml\"))\n",
                "\n",
                "# Fetch data using the client\n",
                "data = get_line_routes(app_id=app_id, app_key=app_key)\n",
                "\n",
                "rows = []\n",
                "target_id = set()\n",
                "\n",
                "for line in data:\n",
                "    line_id = line.get(\"id\")\n",
                "    if not isinstance(line_id, str) or not line_id.isdigit():\n",
                "        continue\n",
                "    \n",
                "    target_id.add(line_id)   \n",
                "    name = line.get(\"name\")\n",
                "    mode = line.get(\"modeName\")\n",
                "\n",
                "    for rs in line.get(\"routeSections\", []):\n",
                "        direction = f\"{rs.get('direction')}\"\n",
                "        originationName = f\"{rs.get('originationName')}\"\n",
                "        destinationName = f\"{rs.get('destinationName')}\"\n",
                "\n",
                "        rows.append({\n",
                "            \"line_id\": line_id,\n",
                "            \"mode\": mode,\n",
                "            \"direction\": direction,\n",
                "            \"origination_name\": originationName,\n",
                "            \"destination_name\": destinationName,\n",
                "        })\n",
                "\n",
                "# --- schema validation ---\n",
                "required = set(schema[\"required_columns\"])\n",
                "for i, row in enumerate(rows):\n",
                "    missing = required - row.keys()\n",
                "    if missing:\n",
                "        raise ValueError(f\"Row {i} missing fields: {missing}\")\n",
                "        \n",
                "output_columns = list(schema[\"output_columns\"].keys())\n",
                "\n",
                "with open(OUTPUT_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
                "    writer = csv.DictWriter(f, fieldnames=output_columns)\n",
                "    writer.writeheader()\n",
                "    writer.writerows(rows)\n",
                "\n",
                "logger.info(f\"Saved {len(rows)} rows to {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "01a89a86",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-13 04:04:22,346 - INFO - Loading stops for lines: ['12', '34']\n",
                        "2026-01-13 04:04:24,835 - INFO - Saved 180 rows to data/reference/line_stop_snapshot/dt=2026-01-13/line_stop.csv and data/reference/line_stop_snapshot/dt=2026-01-13/line_stop.parquet\n"
                    ]
                }
            ],
            "source": [
                "# Preprocess the names and ids of stops on the given lines ids.\n",
                "schema = load_schema(\"schemas/reference/line_stop.yaml\")\n",
                "\n",
                "snapshot_date = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
                "OUTPUT_DIR = Path(f\"data/reference/line_stop_snapshot/dt={snapshot_date}\")\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "rows = []\n",
                "line_ids = config[\"line_ids\"]\n",
                "logger.info(f\"Loading stops for lines: {line_ids}\")\n",
                "\n",
                "for line in line_ids:\n",
                "    inbound, outbound = get_stops_sequence(id=line, app_id=app_id, app_key=app_key)\n",
                "    inbound_routes = inbound.get(\"orderedLineRoutes\", [])\n",
                "    outbound_routes = outbound.get(\"orderedLineRoutes\", [])\n",
                "    \n",
                "    if inbound_routes:\n",
                "        for seq, stop_id in enumerate(inbound_routes[0].get(\"naptanIds\", [])):\n",
                "            rows.append({\n",
                "                \"snapshot_date\": snapshot_date,\n",
                "                \"line_id\": line,\n",
                "                \"direction\": \"inbound\",\n",
                "                \"stop_id\": stop_id,\n",
                "                \"stop_sequence\": seq,\n",
                "            }) # \n",
                "\n",
                "    if outbound_routes:\n",
                "        for seq, stop_id in enumerate(outbound_routes[0].get(\"naptanIds\", [])):\n",
                "            rows.append({\n",
                "                \"snapshot_date\": snapshot_date,\n",
                "                \"line_id\": line,\n",
                "                \"direction\": \"outbound\",\n",
                "                \"stop_id\": stop_id,\n",
                "                \"stop_sequence\": seq,\n",
                "            })\n",
                "\n",
                "# --- schema validation ---\n",
                "required = set(schema[\"required_columns\"])\n",
                "for i, row in enumerate(rows):\n",
                "    missing = required - row.keys()\n",
                "    if missing:\n",
                "        raise ValueError(f\"Row {i} missing fields: {missing}\")\n",
                "\n",
                "output_columns = list(schema[\"output_columns\"].keys())\n",
                "with open(OUTPUT_DIR / f\"line_stop.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
                "    writer = csv.DictWriter(f, fieldnames=output_columns)\n",
                "    writer.writeheader()\n",
                "    writer.writerows(rows)\n",
                "\n",
                "# Save in Parquet\n",
                "df = pd.DataFrame(rows)[output_columns]\n",
                "df.to_parquet(OUTPUT_DIR / f\"line_stop.parquet\", engine=\"pyarrow\")\n",
                "\n",
                "logger.info(f\"Saved {len(rows)} rows to {OUTPUT_DIR / 'line_stop.csv'} and {OUTPUT_DIR / 'line_stop.parquet'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "ce6533aa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-13 04:04:33,565 - INFO - Starting timetable ingestion for 180 line-stop pairs\n",
                        "2026-01-13 04:04:53,332 - INFO - Progress: 50/180 processed | success=50 | skipped=0 | failed=0\n",
                        "2026-01-13 04:05:12,536 - INFO - Progress: 100/180 processed | success=98 | skipped=2 | failed=0\n",
                        "2026-01-13 04:05:30,801 - INFO - Progress: 150/180 processed | success=148 | skipped=2 | failed=0\n",
                        "2026-01-13 04:05:41,392 - INFO - Wrote 65399 rows to data/reference/stop_timetable_snapshot/dt=2026-01-13/stop_timetable.parquet\n",
                        "2026-01-13 04:05:41,393 - INFO - Sample written to stop_timetable_sample.csv\n"
                    ]
                }
            ],
            "source": [
                "# Get the Timetable for every stop on a list of lines\n",
                "snapshot_date = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
                "line_stop_DIR = Path(f\"data/reference/line_stop_snapshot/dt={snapshot_date}\")\n",
                "df = pd.read_parquet(line_stop_DIR/f\"line_stop.parquet\")\n",
                "OUTPUT_DIR = Path(f\"data/reference/stop_timetable_snapshot/dt={snapshot_date}\") \n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "schema = load_schema(Path(\"schemas/reference/stop_timetable.yaml\"))\n",
                "\n",
                "line_stop_pairs = (\n",
                "    df[[\"line_id\", \"stop_id\", \"stop_sequence\"]]\n",
                "    .drop_duplicates(subset=[\"line_id\", \"stop_id\"])\n",
                "    .sort_values([\"line_id\", \"stop_sequence\"], kind=\"stable\")\n",
                ")\n",
                "\n",
                "total_pairs = len(line_stop_pairs)\n",
                "logger.info(f\"Starting timetable ingestion for {total_pairs} line-stop pairs\")\n",
                "processed = 0\n",
                "success = 0\n",
                "skipped = 0\n",
                "failed = 0\n",
                "LOG_EVERY = 50\n",
                "\n",
                "timetable_rows = []\n",
                "\n",
                "for line_id, stop_id, stop_sequence in line_stop_pairs.itertuples(index=False):\n",
                "    processed += 1\n",
                "\n",
                "    try:\n",
                "        timetable_json = get_timetable(\n",
                "            id=line_id,\n",
                "            stop_id=stop_id,\n",
                "            app_id=app_id,\n",
                "            app_key=app_key\n",
                "        )\n",
                "\n",
                "        if timetable_json is None:\n",
                "            skipped += 1\n",
                "            continue\n",
                "\n",
                "        timetable_rows.extend(\n",
                "            extract_timetable_rows(\n",
                "                timetable_json=timetable_json,\n",
                "                snapshot_date=snapshot_date,\n",
                "                stop_sequence=stop_sequence\n",
                "            )\n",
                "        )\n",
                "        success += 1\n",
                "\n",
                "    except Exception as e:\n",
                "        failed += 1\n",
                "        logger.error(\n",
                "            f\"Failed line={line_id}, stop={stop_id}: {e}\"\n",
                "        )\n",
                "\n",
                "    if processed % LOG_EVERY == 0 or processed == total_pairs:\n",
                "        logger.info(\n",
                "            \"Progress: %d/%d processed | success=%d | skipped=%d | failed=%d\",\n",
                "            processed,\n",
                "            total_pairs,\n",
                "            success,\n",
                "            skipped,\n",
                "            failed\n",
                "        )\n",
                "\n",
                "df = pd.DataFrame(timetable_rows)\n",
                "\n",
                "# --- schema validation ---\n",
                "required = set(schema[\"required_columns\"])\n",
                "missing = required - set(df.columns)\n",
                "if missing:\n",
                "    raise ValueError(f\"Missing required columns: {missing}\")\n",
                "\n",
                "output_columns = list(schema[\"output_columns\"].keys())\n",
                "df = df[output_columns]\n",
                "df.to_parquet(OUTPUT_DIR / \"stop_timetable.parquet\", engine=\"pyarrow\")\n",
                "\n",
                "\n",
                "df.head(300).to_csv(OUTPUT_DIR / \"stop_timetable_sample.csv\", index=False)\n",
                "logger.info(f\"Wrote {len(df)} rows to {OUTPUT_DIR/f'stop_timetable.parquet'}\")\n",
                "logger.info(\"Sample written to stop_timetable_sample.csv\")  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50fb3bc1",
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "\n",
                "run_ts = datetime.utcnow()\n",
                "date_part = run_ts.strftime(\"%Y-%m-%d\")\n",
                "hour_part = run_ts.strftime(\"%H\")\n",
                "minute_part = run_ts.strftime(\"%M\")\n",
                "\n",
                "output_dir = Path(f\"Artifacts/predictions/dt={date_part}\")\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "output_file = output_dir / f\"hour={hour_part}_minute={minute_part}.csv\"\n",
                "\n",
                "# Take an example target\n",
                "example_id = config[\"line_ids\"]\n",
                "print(\"example_id: \", example_id) \n",
                "data = get_arrivals(ids=example_id, app_id=app_id, app_key=app_key)\n",
                "\n",
                "rows = []\n",
                "\n",
                "for vehicle in data:\n",
                "    rows.append({\n",
                "        # partition / snapshot metadata\n",
                "        \"prediction_time\": run_ts.isoformat(),\n",
                "\n",
                "        # core identifiers\n",
                "        \"vehicle_id\": vehicle.get(\"vehicleId\"),\n",
                "        \"line_id\": vehicle.get(\"lineId\"),\n",
                "\n",
                "        # stop / location\n",
                "        \"stop_id\": vehicle.get(\"naptanId\"),\n",
                "        \"stop_name\": vehicle.get(\"stationName\"),\n",
                "\n",
                "        # movement\n",
                "        \"direction\": vehicle.get(\"direction\"),\n",
                "        \"destination_name\": vehicle.get(\"destinationName\"),\n",
                "\n",
                "        # KPIs\n",
                "        \"time_to_station_sec\": vehicle.get(\"timeToStation\"),\n",
                "        \"expected_arrival\": vehicle.get(\"expectedArrival\"),\n",
                "    })\n",
                "\n",
                "FIELDNAMES = [\n",
                "    \"prediction_time\",\n",
                "    \"vehicle_id\",\n",
                "    \"line_id\",\n",
                "    \"stop_id\",\n",
                "    \"stop_name\",\n",
                "    \"direction\",\n",
                "    # \"destination_name\",\n",
                "    \"time_to_station_sec\",\n",
                "    \"expected_arrival\",\n",
                "]\n",
                "\n",
                "\n",
                "# if rows:\n",
                "#     with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
                "#         writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n",
                "#         writer.writeheader()\n",
                "#         writer.writerows(rows)\n",
                "\n",
                "rows_df = pd.DataFrame(rows).sort_values([\"line_id\", \"vehicle_id\", \"expected_arrival\"], kind=\"stable\")[FIELDNAMES]\n",
                "rows_df.to_csv(output_file, index=False)\n",
                "\n",
                "print(f\"Saved {len(rows)} rows to {output_file}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a66f5a35",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tfl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
