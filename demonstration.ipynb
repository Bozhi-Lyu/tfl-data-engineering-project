{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1e147d9f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "import logging\n",
                "import csv\n",
                "import time\n",
                "import yaml\n",
                "import os\n",
                "from datetime import datetime,timezone\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "from src.tfl_client import *\n",
                "\n",
                "# Gets all lines and their valid routes for given modes, including the name and id of the originating and terminating stops for each route\n",
                "# GET https://api.tfl.gov.uk/Line/Mode/bus/Route\n",
                "\n",
                "# Get disruptions for all lines of the given modes.\n",
                "# GET https://api.tfl.gov.uk/Line/Mode/bus/Disruption\n",
                "\n",
                "# Get the list of arrival predictions for given line ids based at the given stop\n",
                "# GET https://api.tfl.gov.uk/Line/{lineId}/Arrivals\n",
                "\n",
                "def load_schema(path: str) -> dict:\n",
                "    with open(path, \"r\") as f:\n",
                "        return yaml.safe_load(f)\n",
                "\n",
                "def load_config(path: str) -> dict:\n",
                "    with open(path, \"r\") as f:\n",
                "        return json.load(f)\n",
                "\n",
                "def load_secrets():\n",
                "    app_id = os.getenv(\"TFL_APP_ID\")\n",
                "    app_key = os.getenv(\"TFL_APP_KEY\")\n",
                "\n",
                "    if not app_id or not app_key:\n",
                "        raise RuntimeError(\"Missing TfL credentials: set TFL_APP_ID and TFL_APP_KEY\")\n",
                "    return app_id, app_key\n",
                "\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "logger = logging.getLogger(__name__)\n",
                "app_id, app_key = load_secrets()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "86af32b7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-22 02:31:42,151 - INFO - Saved 1026 rows to data/reference/line_routes_snapshot/dt=2026-01-22/line_routes.parquet\n"
                    ]
                }
            ],
            "source": [
                "# Preprocess the Line routes data\n",
                "snapshot_date = datetime.now(timezone.utc).date().isoformat()\n",
                "OUTPUT_DIR = Path(f\"data/reference/line_routes_snapshot/dt={snapshot_date}\")\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "OUTPUT_FILE = OUTPUT_DIR / \"line_routes.csv\"\n",
                "\n",
                "# Load config, schemas and params\n",
                "config = load_config(Path(\"config/routes.json\"))\n",
                "schema = load_schema(Path(\"schemas/reference/line_routes.yaml\"))\n",
                "\n",
                "# Fetch data using the client\n",
                "data = get_line_routes(app_id=app_id, app_key=app_key)\n",
                "\n",
                "rows = []\n",
                "target_id = set()\n",
                "\n",
                "for line in data:\n",
                "    line_id = line.get(\"id\")\n",
                "    if not isinstance(line_id, str) or not line_id.isdigit():\n",
                "        continue\n",
                "    \n",
                "    target_id.add(line_id)   \n",
                "    name = line.get(\"name\")\n",
                "    mode = line.get(\"modeName\")\n",
                "\n",
                "    for rs in line.get(\"routeSections\", []):\n",
                "        direction = f\"{rs.get('direction')}\"\n",
                "        originationName = f\"{rs.get('originationName')}\"\n",
                "        destinationName = f\"{rs.get('destinationName')}\"\n",
                "\n",
                "        rows.append({\n",
                "            \"line_id\": line_id,\n",
                "            \"mode\": mode,\n",
                "            \"direction\": direction,\n",
                "            \"origination_name\": originationName,\n",
                "            \"destination_name\": destinationName,\n",
                "        })\n",
                "\n",
                "# --- schema validation ---\n",
                "required = set(schema[\"required_columns\"])\n",
                "for i, row in enumerate(rows):\n",
                "    missing = required - row.keys()\n",
                "    if missing:\n",
                "        raise ValueError(f\"Row {i} missing fields: {missing}\")\n",
                "        \n",
                "output_columns = list(schema[\"output_columns\"].keys())\n",
                "\n",
                "df = pd.DataFrame(rows)[output_columns]\n",
                "df.to_parquet(OUTPUT_DIR / \"line_routes.parquet\", engine=\"pyarrow\")\n",
                "\n",
                "SAMPLE_DIR = Path(\"data/reference/line_routes_snapshot/samples\")\n",
                "SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
                "df.head(300).to_csv(\n",
                "    SAMPLE_DIR / f\"line_routes_{snapshot_date}.csv\",\n",
                "    index=False,\n",
                ")\n",
                "\n",
                "logger.info(f\"Saved {len(rows)} rows to {OUTPUT_DIR / 'line_routes.parquet'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "01a89a86",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-22 02:32:00,927 - INFO - Loading stops for lines: ['12', '34']\n",
                        "2026-01-22 02:32:03,437 - INFO - Saved 180 rows to data/reference/line_stop_snapshot/samples/line_stop.csv and data/reference/line_stop_snapshot/dt=2026-01-22/line_stop.parquet\n"
                    ]
                }
            ],
            "source": [
                "# Preprocess the names and ids of stops on the given lines ids.\n",
                "schema = load_schema(\"schemas/reference/line_stop.yaml\")\n",
                "\n",
                "snapshot_date = datetime.now(timezone.utc).date().isoformat()\n",
                "OUTPUT_DIR = Path(f\"data/reference/line_stop_snapshot/dt={snapshot_date}\")\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "rows = []\n",
                "line_ids = config[\"line_ids\"]\n",
                "logger.info(f\"Loading stops for lines: {line_ids}\")\n",
                "\n",
                "for line in line_ids:\n",
                "    inbound, outbound = get_stops_sequence(id=line, app_id=app_id, app_key=app_key)\n",
                "    inbound_routes = inbound.get(\"orderedLineRoutes\", [])\n",
                "    outbound_routes = outbound.get(\"orderedLineRoutes\", [])\n",
                "    \n",
                "    if inbound_routes:\n",
                "        for seq, stop_id in enumerate(inbound_routes[0].get(\"naptanIds\", [])):\n",
                "            rows.append({\n",
                "                \"snapshot_date\": snapshot_date,\n",
                "                \"line_id\": line,\n",
                "                \"direction\": \"inbound\",\n",
                "                \"stop_id\": stop_id,\n",
                "                \"stop_sequence\": seq,\n",
                "            }) # \n",
                "\n",
                "    if outbound_routes:\n",
                "        for seq, stop_id in enumerate(outbound_routes[0].get(\"naptanIds\", [])):\n",
                "            rows.append({\n",
                "                \"snapshot_date\": snapshot_date,\n",
                "                \"line_id\": line,\n",
                "                \"direction\": \"outbound\",\n",
                "                \"stop_id\": stop_id,\n",
                "                \"stop_sequence\": seq,\n",
                "            })\n",
                "\n",
                "# --- schema validation ---\n",
                "required = set(schema[\"required_columns\"])\n",
                "for i, row in enumerate(rows):\n",
                "    missing = required - row.keys()\n",
                "    if missing:\n",
                "        raise ValueError(f\"Row {i} missing fields: {missing}\")\n",
                "\n",
                "output_columns = list(schema[\"output_columns\"].keys())\n",
                "\n",
                "# Save in Parquet\n",
                "df = pd.DataFrame(rows)[output_columns]\n",
                "df.to_parquet(OUTPUT_DIR / f\"line_stop.parquet\", engine=\"pyarrow\")\n",
                "\n",
                "SAMPLE_DIR = Path(\"data/reference/line_stop_snapshot/samples\")\n",
                "SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
                "df.head(300).to_csv(SAMPLE_DIR / f'line_stop_{snapshot_date}.csv', index=False)\n",
                "\n",
                "logger.info(f\"Saved {len(rows)} rows to {SAMPLE_DIR / 'line_stop.csv'} and {OUTPUT_DIR / 'line_stop.parquet'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "ce6533aa",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-22 02:32:12,361 - INFO - Starting timetable ingestion for 180 line-stop pairs\n",
                        "2026-01-22 02:32:20,427 - INFO - Progress: 50/180 processed | success=50 | skipped=0 | failed=0\n",
                        "2026-01-22 02:32:30,071 - INFO - Progress: 100/180 processed | success=98 | skipped=2 | failed=0\n",
                        "2026-01-22 02:32:37,647 - INFO - Progress: 150/180 processed | success=148 | skipped=2 | failed=0\n",
                        "2026-01-22 02:32:42,530 - INFO - Wrote 65399 rows to data/reference/stop_timetable_snapshot/dt=2026-01-22/stop_timetable_2026-01-22.parquet\n",
                        "2026-01-22 02:32:42,531 - INFO - Sample written to data/reference/stop_timetable_snapshot/samples/stop_timetable_2026-01-22.csv\n"
                    ]
                }
            ],
            "source": [
                "# Get the Timetable for every stop on a list of lines\n",
                "snapshot_date = datetime.now(timezone.utc).date().isoformat()\n",
                "line_stop_DIR = Path(f\"data/reference/line_stop_snapshot/dt={snapshot_date}\")\n",
                "df = pd.read_parquet(line_stop_DIR/f\"line_stop.parquet\")\n",
                "OUTPUT_DIR = Path(f\"data/reference/stop_timetable_snapshot/dt={snapshot_date}\") \n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "schema = load_schema(Path(\"schemas/reference/stop_timetable.yaml\"))\n",
                "\n",
                "line_stop_pairs = (\n",
                "    df[[\"line_id\", \"stop_id\", \"stop_sequence\"]]\n",
                "    .drop_duplicates(subset=[\"line_id\", \"stop_id\"])\n",
                "    .sort_values([\"line_id\", \"stop_sequence\"], kind=\"stable\")\n",
                ")\n",
                "\n",
                "total_pairs = len(line_stop_pairs)\n",
                "logger.info(f\"Starting timetable ingestion for {total_pairs} line-stop pairs\")\n",
                "processed = 0\n",
                "success = 0\n",
                "skipped = 0\n",
                "failed = 0\n",
                "LOG_EVERY = 50\n",
                "\n",
                "timetable_rows = []\n",
                "\n",
                "for line_id, stop_id, stop_sequence in line_stop_pairs.itertuples(index=False):\n",
                "    processed += 1\n",
                "\n",
                "    try:\n",
                "        timetable_json = get_timetable(\n",
                "            id=line_id,\n",
                "            stop_id=stop_id,\n",
                "            app_id=app_id,\n",
                "            app_key=app_key\n",
                "        )\n",
                "\n",
                "        if timetable_json is None:\n",
                "            skipped += 1\n",
                "            continue\n",
                "\n",
                "        timetable_rows.extend(\n",
                "            extract_timetable_rows(\n",
                "                timetable_json=timetable_json,\n",
                "                snapshot_date=snapshot_date,\n",
                "                stop_sequence=stop_sequence\n",
                "            )\n",
                "        )\n",
                "        success += 1\n",
                "\n",
                "    except Exception as e:\n",
                "        failed += 1\n",
                "        logger.error(\n",
                "            f\"Failed line={line_id}, stop={stop_id}: {e}\"\n",
                "        )\n",
                "\n",
                "    if processed % LOG_EVERY == 0 or processed == total_pairs:\n",
                "        logger.info(\n",
                "            \"Progress: %d/%d processed | success=%d | skipped=%d | failed=%d\",\n",
                "            processed,\n",
                "            total_pairs,\n",
                "            success,\n",
                "            skipped,\n",
                "            failed\n",
                "        )\n",
                "\n",
                "df = pd.DataFrame(timetable_rows)\n",
                "\n",
                "# --- schema validation ---\n",
                "required = set(schema[\"required_columns\"])\n",
                "missing = required - set(df.columns)\n",
                "if missing:\n",
                "    raise ValueError(f\"Missing required columns: {missing}\")\n",
                "\n",
                "output_columns = list(schema[\"output_columns\"].keys())\n",
                "df = df[output_columns]\n",
                "OUTPUT_FILE = OUTPUT_DIR / f\"stop_timetable_{snapshot_date}.parquet\"\n",
                "df.to_parquet(OUTPUT_FILE, engine=\"pyarrow\")\n",
                "\n",
                "SAMPLE_DIR = Path(\"data/reference/stop_timetable_snapshot/samples\")\n",
                "SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
                "SAMPLE_FILE = SAMPLE_DIR / f\"stop_timetable_{snapshot_date}.csv\"\n",
                "df.head(300).to_csv(SAMPLE_FILE, index=False)\n",
                "\n",
                "logger.info(f\"Wrote {len(df)} rows to {OUTPUT_FILE}\")\n",
                "logger.info(f\"Sample written to {SAMPLE_FILE}\")  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "ca7ef8d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate_bronze_arrival_predictions(df: pd.DataFrame, schema_path: str):\n",
                "    with open(schema_path, \"r\") as f:\n",
                "        schema = yaml.safe_load(f)\n",
                "\n",
                "    columns = schema[\"columns\"]\n",
                "\n",
                "    # 1. Column existence\n",
                "    for col in columns:\n",
                "        if col not in df.columns:\n",
                "            raise ValueError(f\"Missing required column: {col}\")\n",
                "\n",
                "    # 2. Nullability + type checks\n",
                "    for col, meta in columns.items():\n",
                "        if not meta.get(\"nullable\", True):\n",
                "            if df[col].isnull().any():\n",
                "                raise ValueError(f\"Non-nullable column has nulls: {col}\")\n",
                "\n",
                "        if meta[\"type\"] == \"datetime\":\n",
                "            try:\n",
                "                df[col] = pd.to_datetime(df[col], utc=True)\n",
                "            except Exception as e:\n",
                "                raise ValueError(f\"Failed to parse datetime column {col}: {e}\")\n",
                "\n",
                "        if meta[\"type\"] == \"int\":\n",
                "            if not pd.api.types.is_integer_dtype(df[col]):\n",
                "                raise ValueError(f\"Column {col} is not integer typed\")\n",
                "\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "50fb3bc1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-01-22 02:33:40,794 - INFO - Saved 85 rows to data/bronze/arrival_predictions/dt=2026-01-22/part-20260122T013340.parquet\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "example_id:  ['12', '34']\n"
                    ]
                }
            ],
            "source": [
                "# Bronze: Ingestion arrival predictions for given line ids based at the given stop\n",
                "import pyarrow as pa\n",
                "import pyarrow.parquet as pq\n",
                "\n",
                "ingestion_ts = datetime.utcnow()\n",
                "date_part = ingestion_ts.strftime(\"%Y-%m-%d\")\n",
                "hour_part = ingestion_ts.strftime(\"%H\")\n",
                "minute_part = ingestion_ts.strftime(\"%M\")\n",
                "\n",
                "output_dir = Path(f\"data/bronze/arrival_predictions/dt={date_part}\")\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "run_id = ingestion_ts.strftime(\"%Y%m%dT%H%M%S\")\n",
                "output_file = output_dir / f\"part-{run_id}.parquet\"\n",
                "\n",
                "# Take an example target\n",
                "example_id = config[\"line_ids\"]\n",
                "print(\"example_id: \", example_id) \n",
                "data = get_arrivals(ids=example_id, app_id=app_id, app_key=app_key)\n",
                "\n",
                "rows = []\n",
                "\n",
                "for vehicle in data:\n",
                "    rows.append({\n",
                "        # partition / snapshot metadata\n",
                "        \"ingestion_ts\": ingestion_ts.isoformat(),\n",
                "\n",
                "        # core identifiers\n",
                "        \"vehicle_id\": vehicle.get(\"vehicleId\"),\n",
                "        \"line_id\": vehicle.get(\"lineId\"),\n",
                "\n",
                "        # stop / location\n",
                "        \"stop_id\": vehicle.get(\"naptanId\"),\n",
                "        \"stop_name\": vehicle.get(\"stationName\"),\n",
                "\n",
                "        # movement\n",
                "        \"direction\": vehicle.get(\"direction\"),\n",
                "        \"destination_name\": vehicle.get(\"destinationName\"),\n",
                "\n",
                "        # KPIs\n",
                "        \"time_to_station_sec\": vehicle.get(\"timeToStation\"),\n",
                "        \"expected_arrival\": vehicle.get(\"expectedArrival\"), # Event time\n",
                "    })\n",
                "\n",
                "FIELDNAMES = [\n",
                "    \"ingestion_ts\",\n",
                "    \"vehicle_id\",\n",
                "    \"line_id\",\n",
                "    \"stop_id\",\n",
                "    \"stop_name\",\n",
                "    \"direction\",\n",
                "    # \"destination_name\",\n",
                "    \"time_to_station_sec\",\n",
                "    \"expected_arrival\",\n",
                "]\n",
                "\n",
                "rows_df = (\n",
                "    pd.DataFrame(rows)\n",
                "    .sort_values([\"line_id\", \"vehicle_id\", \"expected_arrival\"], kind=\"stable\")\n",
                "    [FIELDNAMES]\n",
                ")\n",
                "\n",
                "# --- schema validation ---\n",
                "rows_df = validate_bronze_arrival_predictions(\n",
                "    rows_df,\n",
                "    \"schemas/bronze/arrival_predictions.yaml\"\n",
                ")\n",
                "\n",
                "table = pa.Table.from_pandas(rows_df, preserve_index=False)\n",
                "pq.write_table(table, output_file)\n",
                "\n",
                "logger.info(f\"Saved {len(rows)} rows to {output_file}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "7ac77c93",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Silver\n",
                "# Keep set-based transformations with window semantics in SQL as the canonical logic.\n",
                "# Python is used only as an orchestration and materialization layer, \n",
                "# which makes the pipeline portable to DuckDB locally and SparkSQL in production.\n",
                "\n",
                "import duckdb\n",
                "from pathlib import Path\n",
                "execution_date = \"2026-01-21\" # Example execution date\n",
                "\n",
                "con = duckdb.connect(database=\":memory:\")\n",
                "\n",
                "bronze_path = (f\"data/bronze/arrival_predictions/dt={execution_date}/*.parquet\")\n",
                "\n",
                "line_stop_path = (\n",
                "    f\"data/reference/line_stop_snapshot/\"\n",
                "    f\"dt={execution_date}/line_stop.parquet\"\n",
                ")\n",
                "\n",
                "sql = Path(\"src/silver/sql/arrival_predictions_route_head.sql\").read_text()\n",
                "\n",
                "sql = sql.replace(\"{{ bronze_path }}\", f\"'{bronze_path}'\")\n",
                "sql = sql.replace(\"{{ line_stop_path }}\", f\"'{line_stop_path}'\")\n",
                "\n",
                "df = con.execute(sql).fetch_df()\n",
                "df.head()\n",
                "\n",
                "output_dir = Path(f\"data/silver/arrival_predictions_route_head/dt={execution_date}\")\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "df.to_parquet(\n",
                "    output_dir / f\"part-{execution_date}.parquet\",\n",
                "    index=False,\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tfl",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
